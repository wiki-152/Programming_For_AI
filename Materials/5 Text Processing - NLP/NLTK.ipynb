{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sMexe4ZL3OSc","executionInfo":{"status":"ok","timestamp":1709699465335,"user_tz":-300,"elapsed":44917,"user":{"displayName":"umair arshad","userId":"05002740496966504818"}},"outputId":"fb102d05-cade-4add-cb6d-5c22505a07b3"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["import nltk\n","nltk.download('all')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c5F7aaHr3OSd","outputId":"a0261cbd-e1ab-429f-e187-c4c9d5ba2589"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Hello Mr. umair, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is blue.', 'You have to eat pizza.']\n"]}],"source":["from nltk.tokenize import sent_tokenize, word_tokenize\n","TEXT = \"Hello Mr. umair, how are you doing today? The weather is great, and Python is awesome. The sky is blue. You have to eat pizza.\"\n","print(sent_tokenize(TEXT))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-YQKlmL3OSe","outputId":"fc5612e2-2e37-4be6-d8d4-3737335f762b"},"outputs":[{"data":{"text/plain":["['Hello',\n"," 'Mr.',\n"," 'umair',\n"," ',',\n"," 'how',\n"," 'are',\n"," 'you',\n"," 'doing',\n"," 'today',\n"," '?',\n"," 'The',\n"," 'weather',\n"," 'is',\n"," 'great',\n"," ',',\n"," 'and',\n"," 'Python',\n"," 'is',\n"," 'awesome',\n"," '.',\n"," 'The',\n"," 'sky',\n"," 'is',\n"," 'blue',\n"," '.',\n"," 'You',\n"," 'have',\n"," 'to',\n"," 'eat',\n"," 'pizza',\n"," '.']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["tokens=word_tokenize(TEXT)\n","tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LDW6oqIc3OSe","outputId":"2b42b60a-000a-4178-b93e-181cb4bf4dda"},"outputs":[{"name":"stdout","output_type":"stream","text":["<Text: Hello Mr. umair , how are you doing...>\n"]}],"source":["from nltk.text import Text\n","t = Text(tokens)\n","print (t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1YfTwpA3OSe","outputId":"6194df27-b24c-414c-9e5f-796eab1a48bd"},"outputs":[{"data":{"text/plain":["3"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["t.count('is')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_WrNABOd3OSe","outputId":"d15ccd44-ef95-4cb4-f695-c1d93bb0b36c"},"outputs":[{"data":{"text/plain":["FreqDist({',': 2,\n","          '.': 3,\n","          '?': 1,\n","          'Ali': 1,\n","          'Hello': 1,\n","          'Mr.': 1,\n","          'Python': 1,\n","          'The': 2,\n","          'You': 1,\n","          'and': 1,\n","          'are': 1,\n","          'awesome': 1,\n","          'blue': 1,\n","          'doing': 1,\n","          'eat': 1,\n","          'great': 1,\n","          'have': 1,\n","          'how': 1,\n","          'is': 3,\n","          'pizza': 1,\n","          'sky': 1,\n","          'to': 1,\n","          'today': 1,\n","          'weather': 1,\n","          'you': 1})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["t.vocab()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J-eR7UO-3OSf"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fh-DaZef3OSf","outputId":"df5b18ba-6ba1-4166-a9b3-a6cd0eb8d9c2"},"outputs":[{"data":{"text/plain":["{'a',\n"," 'about',\n"," 'above',\n"," 'after',\n"," 'again',\n"," 'against',\n"," 'ain',\n"," 'all',\n"," 'am',\n"," 'an',\n"," 'and',\n"," 'any',\n"," 'are',\n"," 'aren',\n"," \"aren't\",\n"," 'as',\n"," 'at',\n"," 'be',\n"," 'because',\n"," 'been',\n"," 'before',\n"," 'being',\n"," 'below',\n"," 'between',\n"," 'both',\n"," 'but',\n"," 'by',\n"," 'can',\n"," 'couldn',\n"," \"couldn't\",\n"," 'd',\n"," 'did',\n"," 'didn',\n"," \"didn't\",\n"," 'do',\n"," 'does',\n"," 'doesn',\n"," \"doesn't\",\n"," 'doing',\n"," 'don',\n"," \"don't\",\n"," 'down',\n"," 'during',\n"," 'each',\n"," 'few',\n"," 'for',\n"," 'from',\n"," 'further',\n"," 'had',\n"," 'hadn',\n"," \"hadn't\",\n"," 'has',\n"," 'hasn',\n"," \"hasn't\",\n"," 'have',\n"," 'haven',\n"," \"haven't\",\n"," 'having',\n"," 'he',\n"," 'her',\n"," 'here',\n"," 'hers',\n"," 'herself',\n"," 'him',\n"," 'himself',\n"," 'his',\n"," 'how',\n"," 'i',\n"," 'if',\n"," 'in',\n"," 'into',\n"," 'is',\n"," 'isn',\n"," \"isn't\",\n"," 'it',\n"," \"it's\",\n"," 'its',\n"," 'itself',\n"," 'just',\n"," 'll',\n"," 'm',\n"," 'ma',\n"," 'me',\n"," 'mightn',\n"," \"mightn't\",\n"," 'more',\n"," 'most',\n"," 'mustn',\n"," \"mustn't\",\n"," 'my',\n"," 'myself',\n"," 'needn',\n"," \"needn't\",\n"," 'no',\n"," 'nor',\n"," 'not',\n"," 'now',\n"," 'o',\n"," 'of',\n"," 'off',\n"," 'on',\n"," 'once',\n"," 'only',\n"," 'or',\n"," 'other',\n"," 'our',\n"," 'ours',\n"," 'ourselves',\n"," 'out',\n"," 'over',\n"," 'own',\n"," 're',\n"," 's',\n"," 'same',\n"," 'shan',\n"," \"shan't\",\n"," 'she',\n"," \"she's\",\n"," 'should',\n"," \"should've\",\n"," 'shouldn',\n"," \"shouldn't\",\n"," 'so',\n"," 'some',\n"," 'such',\n"," 't',\n"," 'than',\n"," 'that',\n"," \"that'll\",\n"," 'the',\n"," 'their',\n"," 'theirs',\n"," 'them',\n"," 'themselves',\n"," 'then',\n"," 'there',\n"," 'these',\n"," 'they',\n"," 'this',\n"," 'those',\n"," 'through',\n"," 'to',\n"," 'too',\n"," 'under',\n"," 'until',\n"," 'up',\n"," 've',\n"," 'very',\n"," 'was',\n"," 'wasn',\n"," \"wasn't\",\n"," 'we',\n"," 'were',\n"," 'weren',\n"," \"weren't\",\n"," 'what',\n"," 'when',\n"," 'where',\n"," 'which',\n"," 'while',\n"," 'who',\n"," 'whom',\n"," 'why',\n"," 'will',\n"," 'with',\n"," 'won',\n"," \"won't\",\n"," 'wouldn',\n"," \"wouldn't\",\n"," 'y',\n"," 'you',\n"," \"you'd\",\n"," \"you'll\",\n"," \"you're\",\n"," \"you've\",\n"," 'your',\n"," 'yours',\n"," 'yourself',\n"," 'yourselves'}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Stopwords\n","from nltk.corpus import stopwords\n","set(stopwords.words('english'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zufVsGhy3OSf","outputId":"8a497195-b561-4ba3-c458-104ea4649ebc"},"outputs":[{"name":"stdout","output_type":"stream","text":["['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n","['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"]}],"source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n","\n","stop_words = set(stopwords.words('english'))\n","\n","word_tokens = word_tokenize(example_sent)\n","\n","# filtered_sentence = [w for w in word_tokens if not w in stop_words]\n","\n","#or\n","filtered_sentence = []\n","\n","for w in word_tokens:\n","    if w not in stop_words:\n","        filtered_sentence.append(w)\n","\n","print(word_tokens)\n","print(filtered_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hudf794D3OSf","outputId":"5e854a70-1822-4e1b-dd62-2fd955018308"},"outputs":[{"data":{"text/plain":["['arabic',\n"," 'azerbaijani',\n"," 'basque',\n"," 'bengali',\n"," 'catalan',\n"," 'chinese',\n"," 'danish',\n"," 'dutch',\n"," 'english',\n"," 'finnish',\n"," 'french',\n"," 'german',\n"," 'greek',\n"," 'hebrew',\n"," 'hinglish',\n"," 'hungarian',\n"," 'indonesian',\n"," 'italian',\n"," 'kazakh',\n"," 'nepali',\n"," 'norwegian',\n"," 'portuguese',\n"," 'romanian',\n"," 'russian',\n"," 'slovene',\n"," 'spanish',\n"," 'swedish',\n"," 'tajik',\n"," 'turkish']"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["stopwords.fileids()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tb_PF7V_3OSf","outputId":"d22cbcb0-ab89-4a44-aaa0-fdfc5b84f34a"},"outputs":[{"data":{"text/plain":["'إذ\\nإذا\\nإذما\\nإذن\\nأف\\nأقل\\nأكثر\\nألا\\nإلا\\nالتي\\nالذي\\nالذين\\nاللاتي\\nاللائي\\nاللتان\\nاللتيا\\nاللتين\\nاللذان\\nاللذين\\nاللواتي\\nإلى\\nإليك\\nإليكم\\nإليكما\\nإليكن\\nأم\\nأما\\nأما\\nإما\\nأن\\nإن\\nإنا\\nأنا\\nأنت\\nأنتم\\nأنتما\\nأنتن\\nإنما\\nإنه\\nأنى\\nأنى\\nآه\\nآها\\nأو\\nأولاء\\nأولئك\\nأوه\\nآي\\nأي\\nأيها\\nإي\\nأين\\nأين\\nأينما\\nإيه\\nبخ\\nبس\\nبعد\\nبعض\\nبك\\nبكم\\nبكم\\nبكما\\nبكن\\nبل\\nبلى\\nبما\\nبماذا\\nبمن\\nبنا\\nبه\\nبها\\nبهم\\nبهما\\nبهن\\nبي\\nبين\\nبيد\\nتلك\\nتلكم\\nتلكما\\nته\\nتي\\nتين\\nتينك\\nثم\\nثمة\\nحاشا\\nحبذا\\nحتى\\nحيث\\nحيثما\\nحين\\nخلا\\nدون\\nذا\\nذات\\nذاك\\nذان\\nذانك\\nذلك\\nذلكم\\nذلكما\\nذلكن\\nذه\\nذو\\nذوا\\nذواتا\\nذواتي\\nذي\\nذين\\nذينك\\nريث\\nسوف\\nسوى\\nشتان\\nعدا\\nعسى\\nعل\\nعلى\\nعليك\\nعليه\\nعما\\nعن\\nعند\\nغير\\nفإذا\\nفإن\\nفلا\\nفمن\\nفي\\nفيم\\nفيما\\nفيه\\nفيها\\nقد\\nكأن\\nكأنما\\nكأي\\nكأين\\nكذا\\nكذلك\\nكل\\nكلا\\nكلاهما\\nكلتا\\nكلما\\nكليكما\\nكليهما\\nكم\\nكم\\nكما\\nكي\\nكيت\\nكيف\\nكيفما\\nلا\\nلاسيما\\nلدى\\nلست\\nلستم\\nلستما\\nلستن\\nلسن\\nلسنا\\nلعل\\nلك\\nلكم\\nلكما\\nلكن\\nلكنما\\nلكي\\nلكيلا\\nلم\\nلما\\nلن\\nلنا\\nله\\nلها\\nلهم\\nلهما\\nلهن\\nلو\\nلولا\\nلوما\\nلي\\nلئن\\nليت\\nليس\\nليسا\\nليست\\nليستا\\nليسوا\\nما\\nماذا\\nمتى\\nمذ\\nمع\\nمما\\nممن\\nمن\\nمنه\\nمنها\\nمنذ\\nمه\\nمهما\\nنحن\\nنحو\\nنعم\\nها\\nهاتان\\nهاته\\nهاتي\\nهاتين\\nهاك\\nهاهنا\\nهذا\\nهذان\\nهذه\\nهذي\\nهذين\\nهكذا\\nهل\\nهلا\\nهم\\nهما\\nهن\\nهنا\\nهناك\\nهنالك\\nهو\\nهؤلاء\\nهي\\nهيا\\nهيت\\nهيهات\\nوالذي\\nوالذين\\nوإذ\\nوإذا\\nوإن\\nولا\\nولكن\\nولو\\nوما\\nومن\\nوهو\\nيا\\nأبٌ\\nأخٌ\\nحمٌ\\nفو\\nأنتِ\\nيناير\\nفبراير\\nمارس\\nأبريل\\nمايو\\nيونيو\\nيوليو\\nأغسطس\\nسبتمبر\\nأكتوبر\\nنوفمبر\\nديسمبر\\nجانفي\\nفيفري\\nمارس\\nأفريل\\nماي\\nجوان\\nجويلية\\nأوت\\nكانون\\nشباط\\nآذار\\nنيسان\\nأيار\\nحزيران\\nتموز\\nآب\\nأيلول\\nتشرين\\nدولار\\nدينار\\nريال\\nدرهم\\nليرة\\nجنيه\\nقرش\\nمليم\\nفلس\\nهللة\\nسنتيم\\nيورو\\nين\\nيوان\\nشيكل\\nواحد\\nاثنان\\nثلاثة\\nأربعة\\nخمسة\\nستة\\nسبعة\\nثمانية\\nتسعة\\nعشرة\\nأحد\\nاثنا\\nاثني\\nإحدى\\nثلاث\\nأربع\\nخمس\\nست\\nسبع\\nثماني\\nتسع\\nعشر\\nثمان\\nسبت\\nأحد\\nاثنين\\nثلاثاء\\nأربعاء\\nخميس\\nجمعة\\nأول\\nثان\\nثاني\\nثالث\\nرابع\\nخامس\\nسادس\\nسابع\\nثامن\\nتاسع\\nعاشر\\nحادي\\nأ\\nب\\nت\\nث\\nج\\nح\\nخ\\nد\\nذ\\nر\\nز\\nس\\nش\\nص\\nض\\nط\\nظ\\nع\\nغ\\nف\\nق\\nك\\nل\\nم\\nن\\nه\\nو\\nي\\nء\\nى\\nآ\\nؤ\\nئ\\nأ\\nة\\nألف\\nباء\\nتاء\\nثاء\\nجيم\\nحاء\\nخاء\\nدال\\nذال\\nراء\\nزاي\\nسين\\nشين\\nصاد\\nضاد\\nطاء\\nظاء\\nعين\\nغين\\nفاء\\nقاف\\nكاف\\nلام\\nميم\\nنون\\nهاء\\nواو\\nياء\\nهمزة\\nي\\nنا\\nك\\nكن\\nه\\nإياه\\nإياها\\nإياهما\\nإياهم\\nإياهن\\nإياك\\nإياكما\\nإياكم\\nإياك\\nإياكن\\nإياي\\nإيانا\\nأولالك\\nتانِ\\nتانِك\\nتِه\\nتِي\\nتَيْنِ\\nثمّ\\nثمّة\\nذانِ\\nذِه\\nذِي\\nذَيْنِ\\nهَؤلاء\\nهَاتانِ\\nهَاتِه\\nهَاتِي\\nهَاتَيْنِ\\nهَذا\\nهَذانِ\\nهَذِه\\nهَذِي\\nهَذَيْنِ\\nالألى\\nالألاء\\nأل\\nأنّى\\nأيّ\\nّأيّان\\nأنّى\\nأيّ\\nّأيّان\\nذيت\\nكأيّ\\nكأيّن\\nبضع\\nفلان\\nوا\\nآمينَ\\nآهِ\\nآهٍ\\nآهاً\\nأُفٍّ\\nأُفٍّ\\nأفٍّ\\nأمامك\\nأمامكَ\\nأوّهْ\\nإلَيْكَ\\nإلَيْكَ\\nإليكَ\\nإليكنّ\\nإيهٍ\\nبخٍ\\nبسّ\\nبَسْ\\nبطآن\\nبَلْهَ\\nحاي\\nحَذارِ\\nحيَّ\\nحيَّ\\nدونك\\nرويدك\\nسرعان\\nشتانَ\\nشَتَّانَ\\nصهْ\\nصهٍ\\nطاق\\nطَق\\nعَدَسْ\\nكِخ\\nمكانَك\\nمكانَك\\nمكانَك\\nمكانكم\\nمكانكما\\nمكانكنّ\\nنَخْ\\nهاكَ\\nهَجْ\\nهلم\\nهيّا\\nهَيْهات\\nوا\\nواهاً\\nوراءَك\\nوُشْكَانَ\\nوَيْ\\nيفعلان\\nتفعلان\\nيفعلون\\nتفعلون\\nتفعلين\\nاتخذ\\nألفى\\nتخذ\\nترك\\nتعلَّم\\nجعل\\nحجا\\nحبيب\\nخال\\nحسب\\nخال\\nدرى\\nرأى\\nزعم\\nصبر\\nظنَّ\\nعدَّ\\nعلم\\nغادر\\nذهب\\nوجد\\nورد\\nوهب\\nأسكن\\nأطعم\\nأعطى\\nرزق\\nزود\\nسقى\\nكسا\\nأخبر\\nأرى\\nأعلم\\nأنبأ\\nحدَث\\nخبَّر\\nنبَّا\\nأفعل به\\nما أفعله\\nبئس\\nساء\\nطالما\\nقلما\\nلات\\nلكنَّ\\nءَ\\nأجل\\nإذاً\\nأمّا\\nإمّا\\nإنَّ\\nأنًّ\\nأى\\nإى\\nأيا\\nب\\nثمَّ\\nجلل\\nجير\\nرُبَّ\\nس\\nعلًّ\\nف\\nكأنّ\\nكلَّا\\nكى\\nل\\nلات\\nلعلَّ\\nلكنَّ\\nلكنَّ\\nم\\nنَّ\\nهلّا\\nوا\\nأل\\nإلّا\\nت\\nك\\nلمّا\\nن\\nه\\nو\\nا\\nي\\nتجاه\\nتلقاء\\nجميع\\nحسب\\nسبحان\\nشبه\\nلعمر\\nمثل\\nمعاذ\\nأبو\\nأخو\\nحمو\\nفو\\nمئة\\nمئتان\\nثلاثمئة\\nأربعمئة\\nخمسمئة\\nستمئة\\nسبعمئة\\nثمنمئة\\nتسعمئة\\nمائة\\nثلاثمائة\\nأربعمائة\\nخمسمائة\\nستمائة\\nسبعمائة\\nثمانمئة\\nتسعمائة\\nعشرون\\nثلاثون\\nاربعون\\nخمسون\\nستون\\nسبعون\\nثمانون\\nتسعون\\nعشرين\\nثلاثين\\nاربعين\\nخمسين\\nستين\\nسبعين\\nثمانين\\nتسعين\\nبضع\\nنيف\\nأجمع\\nجميع\\nعامة\\nعين\\nنفس\\nلا سيما\\nأصلا\\nأهلا\\nأيضا\\nبؤسا\\nبعدا\\nبغتة\\nتعسا\\nحقا\\nحمدا\\nخلافا\\nخاصة\\nدواليك\\nسحقا\\nسرا\\nسمعا\\nصبرا\\nصدقا\\nصراحة\\nطرا\\nعجبا\\nعيانا\\nغالبا\\nفرادى\\nفضلا\\nقاطبة\\nكثيرا\\nلبيك\\nمعاذ\\nأبدا\\nإزاء\\nأصلا\\nالآن\\nأمد\\nأمس\\nآنفا\\nآناء\\nأنّى\\nأول\\nأيّان\\nتارة\\nثمّ\\nثمّة\\nحقا\\nصباح\\nمساء\\nضحوة\\nعوض\\nغدا\\nغداة\\nقطّ\\nكلّما\\nلدن\\nلمّا\\nمرّة\\nقبل\\nخلف\\nأمام\\nفوق\\nتحت\\nيمين\\nشمال\\nارتدّ\\nاستحال\\nأصبح\\nأضحى\\nآض\\nأمسى\\nانقلب\\nبات\\nتبدّل\\nتحوّل\\nحار\\nرجع\\nراح\\nصار\\nظلّ\\nعاد\\nغدا\\nكان\\nما انفك\\nما برح\\nمادام\\nمازال\\nمافتئ\\nابتدأ\\nأخذ\\nاخلولق\\nأقبل\\nانبرى\\nأنشأ\\nأوشك\\nجعل\\nحرى\\nشرع\\nطفق\\nعلق\\nقام\\nكرب\\nكاد\\nهبّ'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["stopwords.raw('arabic')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HsBJjWRl3OSf","outputId":"77aea41a-092a-4b06-aac0-c4a551dc39fd"},"outputs":[{"ename":"OSError","evalue":"No such file or directory: 'C:\\\\Users\\\\Umair\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\romanurdu'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_620/4015281545.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'romanurdu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m                 \u001b[0mcontents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \"\"\"\n\u001b[0;32m    229\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file or directory: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mOSError\u001b[0m: No such file or directory: 'C:\\\\Users\\\\Umair\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\romanurdu'"]}],"source":["stopwords.raw('romanurdu').replace('\\n', ' ')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1n0tAu63OSf"},"outputs":[],"source":["from nltk.stem import PorterStemmer\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","ps = PorterStemmer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p1Ixavh33OSg","outputId":"93557463-04c1-41fc-f891-59a4aaf2cc36"},"outputs":[{"name":"stdout","output_type":"stream","text":["python\n","python\n","python\n","python\n"]}],"source":["example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\"]\n","for w in example_words:\n","    print(ps.stem(w))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25ez3olp3OSg"},"outputs":[],"source":["new_text = \"It is Went to by worse pythoner womens while you are pythoning with python. All pythoners have pythoned poorly at least once. there are foxes\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ka3iZJ4-3OSg","outputId":"bb60129b-131c-4949-d755-d10a6fe04b9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["it\n","is\n","went\n","to\n","by\n","wors\n","python\n","women\n","while\n","you\n","are\n","python\n","with\n","python\n",".\n","all\n","python\n","have\n","python\n","poorli\n","at\n","least\n","onc\n",".\n","there\n","are\n","fox\n"]}],"source":["words = word_tokenize(new_text)\n","\n","for w in words:\n","    print(ps.stem(w))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46bdYHHm3OSg","outputId":"1258265c-1442-4316-a993-36653feccdf3"},"outputs":[{"name":"stdout","output_type":"stream","text":["It\n","is\n","Went\n","to\n","by\n","worse\n","pythoner\n","woman\n","while\n","you\n","are\n","pythoning\n","with\n","python\n",".\n","All\n","pythoners\n","have\n","pythoned\n","poorly\n","at\n","least\n","once\n",".\n","there\n","are\n","fox\n"]}],"source":["from nltk import WordNetLemmatizer\n","\n","wnl = WordNetLemmatizer()\n","words = word_tokenize(new_text)\n","\n","for w in words:\n","    print(wnl.lemmatize(w))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zpx4H-rU3OSg","outputId":"4f992b2a-6eef-4d70-b31d-0903eae3c105"},"outputs":[{"name":"stdout","output_type":"stream","text":["the women run in the fog pass bunni work as comput scientist .\n","the wom run in the fog pass bunny work as comput sci .\n","the women run in the fog pass bunni work as comput scientist .\n"]}],"source":["#Different stemmers\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.stem.lancaster import LancasterStemmer\n","from nltk.stem.porter import PorterStemmer\n","\n","text = list(nltk.word_tokenize(\"The women running in the fog passed bunnies working as computer scientists.\"))\n","\n","snowball = SnowballStemmer('english')\n","lancaster = LancasterStemmer()\n","porter = PorterStemmer()\n","\n","for stemmer in (snowball, lancaster, porter):\n","    stemmed_text = [stemmer.stem(t) for t in text]\n","    print (\" \".join(stemmed_text))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AGpzNhVJ3OSg","outputId":"176b84b2-9e8a-4434-a7f2-782796586e6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["['eagle', 'fly', 'midnight']\n"]}],"source":["#Typical normalization of text for use as features in machine learning models looks something like this:\n","import string\n","lemmatizer  = WordNetLemmatizer()\n","stopwords   = set(nltk.corpus.stopwords.words('english'))\n","punctuation = string.punctuation\n","def normalize(text):\n","    for token in nltk.word_tokenize(text):\n","        token = token.lower()\n","        token = lemmatizer.lemmatize(token)\n","        if token not in stopwords and token not in punctuation:\n","            yield token\n","\n","print (list(normalize(\"The EAGLE flies at midnight.\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q4MvVumi3OSg","outputId":"bc5bc18c-c4fe-4639-f452-badc62ee42d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["The women running in the four passed bundles working as computer scientists.\n"]}],"source":["from textblob import TextBlob\n","\n","\n","text = \"The women runnning in the foug passed bunnies working as computer scientists.\"               # Reading the file\n","textBlb = TextBlob(text)            # Making our first textblob\n","textCorrected = textBlb.correct()   # Correcting the text\n","print(textCorrected)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vp3DkEXK3OSg","outputId":"e0521689-9cd4-4017-e76c-7742452947e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"]}],"source":["import string\n","punctuation = string.punctuation\n","print(punctuation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ojgnq1-E3OSg"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}